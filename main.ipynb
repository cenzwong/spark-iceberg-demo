{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 15:37:16 WARN Utils: Your hostname, codespaces-7465f4 resolves to a loopback address: 127.0.0.1; using 10.0.1.210 instead (on interface eth0)\n",
      "25/01/20 15:37:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3b019e13-5f39-4a38-94a1-08d3b7d8288d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      ":: resolution report :: resolve 179ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3b019e13-5f39-4a38-94a1-08d3b7d8288d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/4ms)\n",
      "25/01/20 15:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "# spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\\\n",
    "#     --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n",
    "#     --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\n",
    "#     --conf spark.sql.catalog.spark_catalog.type=hive \\\n",
    "#     --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \\\n",
    "#     --conf spark.sql.catalog.local.type=hadoop \\\n",
    "#     --conf spark.sql.catalog.local.warehouse=$PWD/warehouse\n",
    "\n",
    "# spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\n",
    "\n",
    "\n",
    "spark_configs = {\n",
    "    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.catalog.spark_catalog\": \"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "    \"spark.sql.catalog.spark_catalog.type\": \"hive\",\n",
    "    \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.local.warehouse\":\"./warehouse\",\n",
    "    \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "}\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark SQL basic example\")\n",
    "    .config(map=spark_configs)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TABLE local.db.table (id bigint, data string) USING iceberg;\")\n",
    "spark.read.table(\"local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c');\")\n",
    "spark.read.table(\"local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  0|mydata|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).withColumn(\"data\", F.lit(\"mydata\")).select(\"id\", \"data\").writeTo(\"local.db.table\").append()\n",
    "spark.read.table(\"local.db.table\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 15:37:...|8492778904122201325|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:37:...|6031520081634296825|8492778904122201325|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time Travel\n",
    "\n",
    "(spark.read\n",
    "    .option(\"snapshot-id\", 8492778904122201325)\n",
    "    .table(\"local.db.table\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                  id|              bigint|   NULL|\n",
      "|                data|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|  # Metadata Columns|                    |       |\n",
      "|            _spec_id|                 int|       |\n",
      "|          _partition|            struct<>|       |\n",
      "|               _file|              string|       |\n",
      "|                _pos|              bigint|       |\n",
      "|            _deleted|             boolean|       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|      local.db.table|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Location|./warehouse/db/table|       |\n",
      "|            Provider|             iceberg|       |\n",
      "|               Owner|           codespace|       |\n",
      "|    Table Properties|[current-snapshot...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "DESCRIBE EXTENDED local.db.table\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|   bigint|   NULL|\n",
      "|    data|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "DESCRIBE local.db.table\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                 key|              value|\n",
      "+--------------------+-------------------+\n",
      "| current-snapshot-id|6031520081634296825|\n",
      "|              format|    iceberg/parquet|\n",
      "|      format-version|                  2|\n",
      "|write.parquet.com...|               zstd|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TBLPROPERTIES local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  0|mydata|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"./warehouse/db/table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 15:37:...|8492778904122201325|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:37:...|6031520081634296825|8492778904122201325|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  0|mydata|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    "  .format(\"iceberg\")\n",
    "  .option(\"start-snapshot-id\", \"8492778904122201325\")\n",
    "  .option(\"end-snapshot-id\", \"6031520081634296825\")\n",
    "  .load(\"./warehouse/db/table\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 15:37:...|8492778904122201325|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:37:...|6031520081634296825|8492778904122201325|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-01-20 15:37:...|8492778904122201325|               NULL|               true|\n",
      "|2025-01-20 15:37:...|6031520081634296825|8492778904122201325|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|status|        snapshot_id|sequence_number|file_sequence_number|           data_file|    readable_metrics|\n",
      "+------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|     1|6031520081634296825|              2|                   2|{0, warehouse/db/...|{{72, 5, 0, NULL,...|\n",
      "|     1|6031520081634296825|              2|                   2|{0, warehouse/db/...|{{72, 5, 0, NULL,...|\n",
      "|     1|8492778904122201325|              1|                   1|{0, warehouse/db/...|{{37, 1, 0, NULL,...|\n",
      "|     1|8492778904122201325|              1|                   1|{0, warehouse/db/...|{{42, 2, 0, NULL,...|\n",
      "+------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.entries\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------+----------------------------------------------------------+------------+-------------+------------+-------------+----------------------------------------------------------+\n",
      "|content|file_path                                                                           |file_format|spec_id|record_count|file_size_in_bytes|column_sizes      |value_counts    |null_value_counts|nan_value_counts|lower_bounds                                              |upper_bounds                                              |key_metadata|split_offsets|equality_ids|sort_order_id|readable_metrics                                          |\n",
      "+-------+------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------+----------------------------------------------------------+------------+-------------+------------+-------------+----------------------------------------------------------+\n",
      "|0      |warehouse/db/table/data/00000-5-90c4a71a-a56f-4da2-a049-52bcf0463765-0-00001.parquet|PARQUET    |0      |5           |720               |{1 -> 55, 2 -> 72}|{1 -> 5, 2 -> 5}|{1 -> 0, 2 -> 0} |{}              |{1 -> [00 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|{1 -> [04 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|NULL        |[4]          |NULL        |0            |{{72, 5, 0, NULL, mydata, mydata}, {55, 5, 0, NULL, 0, 4}}|\n",
      "|0      |warehouse/db/table/data/00001-6-90c4a71a-a56f-4da2-a049-52bcf0463765-0-00001.parquet|PARQUET    |0      |5           |720               |{1 -> 55, 2 -> 72}|{1 -> 5, 2 -> 5}|{1 -> 0, 2 -> 0} |{}              |{1 -> [05 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|{1 -> [09 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|NULL        |[4]          |NULL        |0            |{{72, 5, 0, NULL, mydata, mydata}, {55, 5, 0, NULL, 5, 9}}|\n",
      "|0      |warehouse/db/table/data/00000-2-631c2dac-e478-4f9b-a526-5b9ed1f61ece-0-00001.parquet|PARQUET    |0      |1           |626               |{1 -> 40, 2 -> 37}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [01 00 00 00 00 00 00 00], 2 -> [61]}               |{1 -> [01 00 00 00 00 00 00 00], 2 -> [61]}               |NULL        |[4]          |NULL        |0            |{{37, 1, 0, NULL, a, a}, {40, 1, 0, NULL, 1, 1}}          |\n",
      "|0      |warehouse/db/table/data/00001-3-631c2dac-e478-4f9b-a526-5b9ed1f61ece-0-00001.parquet|PARQUET    |0      |2           |634               |{1 -> 48, 2 -> 42}|{1 -> 2, 2 -> 2}|{1 -> 0, 2 -> 0} |{}              |{1 -> [02 00 00 00 00 00 00 00], 2 -> [62]}               |{1 -> [03 00 00 00 00 00 00 00], 2 -> [63]}               |NULL        |[4]          |NULL        |0            |{{42, 2, 0, NULL, b, c}, {48, 2, 0, NULL, 2, 3}}          |\n",
      "+-------+------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------+----------------------------------------------------------+------------+-------------+------------+-------------+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.files\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|13          |4         |2700                         |0                           |0                         |0                           |0                         |2025-01-20 15:37:27.878|6031520081634296825     |\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name|type  |snapshot_id        |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main|BRANCH|6031520081634296825|NULL                   |NULL                 |NULL                  |\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.refs\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|id |data  |\n",
      "+---+------+\n",
      "|0  |mydata|\n",
      "|1  |mydata|\n",
      "|2  |mydata|\n",
      "|3  |mydata|\n",
      "|4  |mydata|\n",
      "|5  |mydata|\n",
      "|6  |mydata|\n",
      "|7  |mydata|\n",
      "|8  |mydata|\n",
      "|9  |mydata|\n",
      "|1  |a     |\n",
      "|2  |b     |\n",
      "|3  |c     |\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "UPDATE local.db.table\n",
    "SET data = 'hello'\n",
    "WHERE id=0\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|id |data  |\n",
      "+---+------+\n",
      "|1  |a     |\n",
      "|2  |b     |\n",
      "|3  |c     |\n",
      "|5  |mydata|\n",
      "|6  |mydata|\n",
      "|7  |mydata|\n",
      "|8  |mydata|\n",
      "|9  |mydata|\n",
      "|0  |hello |\n",
      "|1  |mydata|\n",
      "|2  |mydata|\n",
      "|3  |mydata|\n",
      "|4  |mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 15:39:38 WARN HadoopTableOperations: Error reading version hint file warehouse/db/table2/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File warehouse/db/table2/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:317)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:103)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:368)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/20 15:39:38 WARN HadoopTableOperations: Error reading version hint file warehouse/db/table2/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File warehouse/db/table2/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:317)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:103)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:83)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:377)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).writeTo(\"local.db.table2\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                   |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+-----------------------+-------------------+-------------------+---------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-01-20 15:37:26.666|8492778904122201325|NULL               |append   |warehouse/db/table/metadata/snap-8492778904122201325-1-6c2133c8-1278-4db8-be5a-65de112f22c9.avro|{spark.app.id -> local-1737387438351, added-data-files -> 2, added-records -> 3, added-files-size -> 1260, changed-partition-count -> 1, total-records -> 3, total-files-size -> 1260, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1737387438351, engine-name -> spark, iceberg-version -> Apache Iceberg 1.7.1 (commit 4a432839233f2343a9eae8255532f911f06358ef)}                                                                          |\n",
      "|2025-01-20 15:37:27.878|6031520081634296825|8492778904122201325|append   |warehouse/db/table/metadata/snap-6031520081634296825-1-30de9d5a-d06a-41b1-976c-74d8aa262d7a.avro|{spark.app.id -> local-1737387438351, added-data-files -> 2, added-records -> 10, added-files-size -> 1440, changed-partition-count -> 1, total-records -> 13, total-files-size -> 2700, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1737387438351, engine-name -> spark, iceberg-version -> Apache Iceberg 1.7.1 (commit 4a432839233f2343a9eae8255532f911f06358ef)}                                                                        |\n",
      "|2025-01-20 15:39:25.388|7201136700195168713|6031520081634296825|overwrite|warehouse/db/table/metadata/snap-7201136700195168713-1-538203c8-1f2f-4229-86be-f466d26a8f6f.avro|{spark.app.id -> local-1737387438351, added-data-files -> 1, deleted-data-files -> 1, added-records -> 5, deleted-records -> 5, added-files-size -> 733, removed-files-size -> 720, changed-partition-count -> 1, total-records -> 13, total-files-size -> 2713, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1737387438351, engine-name -> spark, iceberg-version -> Apache Iceberg 1.7.1 (commit 4a432839233f2343a9eae8255532f911f06358ef)}|\n",
      "+-----------------------+-------------------+-------------------+---------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CALL local.system.expire_snapshots(table => 'db.table', snapshot_ids => ARRAY(8492778904122201325))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 15:37:...|6031520081634296825|8492778904122201325|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:39:...|7201136700195168713|6031520081634296825|overwrite|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(2).withColumn(\"data\", F.lit(\"hi\")).select(\"id\", \"data\").writeTo(\"local.db.table\").append()\n",
    "spark.range(2).withColumn(\"data\", F.lit(\"hi2\")).select(\"id\", \"data\").writeTo(\"local.db.table\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  0|    hi|\n",
      "|  1|    hi|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "|  0|   hi2|\n",
      "|  1|   hi2|\n",
      "|  0| hello|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 15:37:...|6031520081634296825|8492778904122201325|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:39:...|7201136700195168713|6031520081634296825|overwrite|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:40:...| 749386769863266795|7201136700195168713|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 15:40:...|6938968765761808873| 749386769863266795|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|previous_snapshot_id|current_snapshot_id|\n",
      "+--------------------+-------------------+\n",
      "| 6938968765761808873|7201136700195168713|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CALL local.system.rollback_to_snapshot('db.table', 7201136700195168713)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  0| hello|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
