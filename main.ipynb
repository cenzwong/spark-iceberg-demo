{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 11:39:57 WARN Utils: Your hostname, codespaces-7465f4 resolves to a loopback address: 127.0.0.1; using 10.0.1.202 instead (on interface eth0)\n",
      "25/01/20 11:39:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2122c437-a131-4bd6-bdfe-6f3c84979576;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      ":: resolution report :: resolve 130ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2122c437-a131-4bd6-bdfe-6f3c84979576\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/12ms)\n",
      "25/01/20 11:39:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "# spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\\\n",
    "#     --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n",
    "#     --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\n",
    "#     --conf spark.sql.catalog.spark_catalog.type=hive \\\n",
    "#     --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \\\n",
    "#     --conf spark.sql.catalog.local.type=hadoop \\\n",
    "#     --conf spark.sql.catalog.local.warehouse=$PWD/warehouse\n",
    "\n",
    "# spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\n",
    "\n",
    "\n",
    "spark_configs = {\n",
    "    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.catalog.spark_catalog\": \"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "    \"spark.sql.catalog.spark_catalog.type\": \"hive\",\n",
    "    \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.local.warehouse\":\"./warehouse\",\n",
    "    \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "}\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark SQL basic example\")\n",
    "    .config(map=spark_configs)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TABLE local.db.table (id bigint, data string) USING iceberg;\")\n",
    "spark.read.table(\"local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c');\")\n",
    "spark.read.table(\"local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  0|mydata|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).withColumn(\"data\", F.lit(\"mydata\")).select(\"id\", \"data\").writeTo(\"local.db.table\").append()\n",
    "spark.read.table(\"local.db.table\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 11:17:...|6525315697047048531|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:21:...|2719531211674363846|6525315697047048531|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:21:...|7517947901201637501|2719531211674363846|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:39:...|9090593959980706118|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:39:...|8399356804968829315|9090593959980706118|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:40:...|3389023457785785759|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:40:...| 111972506823236605|3389023457785785759|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time Travel\n",
    "\n",
    "(spark.read\n",
    "    .option(\"snapshot-id\", 6525315697047048531)\n",
    "    .table(\"local.db.table\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                  id|              bigint|   NULL|\n",
      "|                data|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|  # Metadata Columns|                    |       |\n",
      "|            _spec_id|                 int|       |\n",
      "|          _partition|            struct<>|       |\n",
      "|               _file|              string|       |\n",
      "|                _pos|              bigint|       |\n",
      "|            _deleted|             boolean|       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|      local.db.table|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Location|./warehouse/db/table|       |\n",
      "|            Provider|             iceberg|       |\n",
      "|               Owner|           codespace|       |\n",
      "|    Table Properties|[current-snapshot...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "DESCRIBE EXTENDED local.db.table\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|   bigint|   NULL|\n",
      "|    data|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 11:40:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "DESCRIBE local.db.table\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                 key|             value|\n",
      "+--------------------+------------------+\n",
      "| current-snapshot-id|111972506823236605|\n",
      "|              format|   iceberg/parquet|\n",
      "|      format-version|                 2|\n",
      "|write.parquet.com...|              zstd|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TBLPROPERTIES local.db.table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  0|mydata|\n",
      "|  1|mydata|\n",
      "|  2|mydata|\n",
      "|  3|mydata|\n",
      "|  4|mydata|\n",
      "|  5|mydata|\n",
      "|  6|mydata|\n",
      "|  7|mydata|\n",
      "|  8|mydata|\n",
      "|  9|mydata|\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"./warehouse/db/table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    "  .format(\"iceberg\")\n",
    "  .option(\"start-snapshot-id\", \"9090593959980706118\")\n",
    "  .option(\"end-snapshot-id\", \"8399356804968829315\")\n",
    "  .load(\"./warehouse/db/table\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-01-20 11:17:...|6525315697047048531|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:21:...|2719531211674363846|6525315697047048531|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:21:...|7517947901201637501|2719531211674363846|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:39:...|9090593959980706118|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:39:...|8399356804968829315|9090593959980706118|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:40:...|3389023457785785759|               NULL|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "|2025-01-20 11:40:...| 111972506823236605|3389023457785785759|   append|warehouse/db/tabl...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.snapshots\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-01-20 11:17:...|6525315697047048531|               NULL|              false|\n",
      "|2025-01-20 11:21:...|2719531211674363846|6525315697047048531|              false|\n",
      "|2025-01-20 11:21:...|7517947901201637501|2719531211674363846|              false|\n",
      "|2025-01-20 11:39:...|9090593959980706118|               NULL|              false|\n",
      "|2025-01-20 11:39:...|8399356804968829315|9090593959980706118|              false|\n",
      "|2025-01-20 11:40:...|3389023457785785759|               NULL|               true|\n",
      "|2025-01-20 11:40:...| 111972506823236605|3389023457785785759|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|status|        snapshot_id|sequence_number|file_sequence_number|           data_file|    readable_metrics|\n",
      "+------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|     1| 111972506823236605|              7|                   7|{0, warehouse/db/...|{{72, 5, 0, NULL,...|\n",
      "|     1| 111972506823236605|              7|                   7|{0, warehouse/db/...|{{72, 5, 0, NULL,...|\n",
      "|     1|3389023457785785759|              6|                   6|{0, warehouse/db/...|{{37, 1, 0, NULL,...|\n",
      "|     1|3389023457785785759|              6|                   6|{0, warehouse/db/...|{{42, 2, 0, NULL,...|\n",
      "+------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.entries\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------+----------------------------------------------------------+------------+-------------+------------+-------------+----------------------------------------------------------+\n",
      "|content|file_path                                                                           |file_format|spec_id|record_count|file_size_in_bytes|column_sizes      |value_counts    |null_value_counts|nan_value_counts|lower_bounds                                              |upper_bounds                                              |key_metadata|split_offsets|equality_ids|sort_order_id|readable_metrics                                          |\n",
      "+-------+------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------+----------------------------------------------------------+------------+-------------+------------+-------------+----------------------------------------------------------+\n",
      "|0      |warehouse/db/table/data/00000-5-72535e5f-c420-42e2-a751-710c95d1f633-0-00001.parquet|PARQUET    |0      |5           |720               |{1 -> 55, 2 -> 72}|{1 -> 5, 2 -> 5}|{1 -> 0, 2 -> 0} |{}              |{1 -> [00 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|{1 -> [04 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|NULL        |[4]          |NULL        |0            |{{72, 5, 0, NULL, mydata, mydata}, {55, 5, 0, NULL, 0, 4}}|\n",
      "|0      |warehouse/db/table/data/00001-6-72535e5f-c420-42e2-a751-710c95d1f633-0-00001.parquet|PARQUET    |0      |5           |720               |{1 -> 55, 2 -> 72}|{1 -> 5, 2 -> 5}|{1 -> 0, 2 -> 0} |{}              |{1 -> [05 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|{1 -> [09 00 00 00 00 00 00 00], 2 -> [6D 79 64 61 74 61]}|NULL        |[4]          |NULL        |0            |{{72, 5, 0, NULL, mydata, mydata}, {55, 5, 0, NULL, 5, 9}}|\n",
      "|0      |warehouse/db/table/data/00000-2-aa49ad4d-2b0a-48dc-a1e1-7ed1c8c55597-0-00001.parquet|PARQUET    |0      |1           |626               |{1 -> 40, 2 -> 37}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [01 00 00 00 00 00 00 00], 2 -> [61]}               |{1 -> [01 00 00 00 00 00 00 00], 2 -> [61]}               |NULL        |[4]          |NULL        |0            |{{37, 1, 0, NULL, a, a}, {40, 1, 0, NULL, 1, 1}}          |\n",
      "|0      |warehouse/db/table/data/00001-3-aa49ad4d-2b0a-48dc-a1e1-7ed1c8c55597-0-00001.parquet|PARQUET    |0      |2           |634               |{1 -> 48, 2 -> 42}|{1 -> 2, 2 -> 2}|{1 -> 0, 2 -> 0} |{}              |{1 -> [02 00 00 00 00 00 00 00], 2 -> [62]}               |{1 -> [03 00 00 00 00 00 00 00], 2 -> [63]}               |NULL        |[4]          |NULL        |0            |{{42, 2, 0, NULL, b, c}, {48, 2, 0, NULL, 2, 3}}          |\n",
      "+-------+------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------+----------------------------------------------------------+------------+-------------+------------+-------------+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.files\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|13          |4         |2700                         |0                           |0                         |0                           |0                         |2025-01-20 11:40:07.752|111972506823236605      |\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.partitions\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------------+-----------------------+---------------------+----------------------+\n",
      "|name|type  |snapshot_id       |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+----+------+------------------+-----------------------+---------------------+----------------------+\n",
      "|main|BRANCH|111972506823236605|NULL                   |NULL                 |NULL                  |\n",
      "+----+------+------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table.refs\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|id |data  |\n",
      "+---+------+\n",
      "|0  |mydata|\n",
      "|1  |mydata|\n",
      "|2  |mydata|\n",
      "|3  |mydata|\n",
      "|4  |mydata|\n",
      "|5  |mydata|\n",
      "|6  |mydata|\n",
      "|7  |mydata|\n",
      "|8  |mydata|\n",
      "|9  |mydata|\n",
      "|1  |a     |\n",
      "|2  |b     |\n",
      "|3  |c     |\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "UPDATE local.db.table\n",
    "SET data = 'hello'\n",
    "WHERE id=0\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|id |data  |\n",
      "+---+------+\n",
      "|0  |hello |\n",
      "|1  |mydata|\n",
      "|2  |mydata|\n",
      "|3  |mydata|\n",
      "|4  |mydata|\n",
      "|5  |mydata|\n",
      "|6  |mydata|\n",
      "|7  |mydata|\n",
      "|8  |mydata|\n",
      "|9  |mydata|\n",
      "|1  |a     |\n",
      "|2  |b     |\n",
      "|3  |c     |\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(10).writeTo(\"local.db.table2\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"local.db.table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
